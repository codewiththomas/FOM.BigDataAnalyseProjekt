# ResearchRAG - Modulares RAG-System

Ein flexibles und erweiterbares RAG-System fÃ¼r wissenschaftliche Studien, das es ermÃ¶glicht, verschiedene Komponenten (Chunker, Embeddings, Vector Stores, Language Models) einfach auszutauschen und zu vergleichen.

## ğŸ¯ ProjektÃ¼bersicht

Dieses Projekt wurde fÃ¼r eine wissenschaftliche Studie (Hausarbeit; 4 Personen; 30 Tage) entwickelt, bei der jede Person einen spezifischen Aspekt des RAG-Systems untersucht:

- **Person A**: Chunking-Strategien (verschiedene Chunker-Implementierungen)
- **Person B**: Embedding-Verfahren (verschiedene Embedding-Modelle)
- **Person C**: Vector Stores & Retrieval (verschiedene Speicher- und Suchstrategien)
- **Person D**: Language Models & Generation (verschiedene LLM-AnsÃ¤tze)

## ğŸ—ï¸ Architektur

### Modularer Aufbau
```
src/
â”œâ”€â”€ components/           # Austauschbare RAG-Komponenten
â”‚   â”œâ”€â”€ chunkers/        # Text-Chunking-Strategien
â”‚   â”œâ”€â”€ embeddings/      # Embedding-Modelle
â”‚   â”œâ”€â”€ vector_stores/   # Vector-Speicher-Systeme
â”‚   â””â”€â”€ language_models/ # Language Models
â”œâ”€â”€ config/              # Konfigurationssystem
â”œâ”€â”€ core/                # RAG-Pipeline und Component Loader
â”œâ”€â”€ evaluations/         # Evaluierungsmetriken
â””â”€â”€ utils/               # Hilfsfunktionen
```

### Datenstruktur
```
data/
â”œâ”€â”€ raw/                 # Rohdaten (DSGVO-Text)
â”œâ”€â”€ processed/           # Verarbeitete Daten
â”‚   â”œâ”€â”€ chunks/         # Gespeicherte Chunks
â”‚   â””â”€â”€ embeddings/     # Gespeicherte Embeddings
â””â”€â”€ evaluation/         # Evaluierungsdaten
    â”œâ”€â”€ results/        # Experiment-Ergebnisse
    â””â”€â”€ qa_pairs.json   # QA-Datensatz
```

## ğŸš€ Schnellstart

### 1. Installation

```bash
# Repository klonen
git clone <repository-url>
cd FOM.BigDataAnalyseProjekt

# AbhÃ¤ngigkeiten installieren
pip install -r requirements.txt

# Paket im Entwicklungsmodus installieren (empfohlen)
pip install -e .
```

**Ohne Installation (Portabel):**
```bash
# Projekt in beliebiges Verzeichnis kopieren
cp -r FOM.BigDataAnalyseProjekt /mycode
cd /mycode

# AbhÃ¤ngigkeiten installieren
pip install -r requirements.txt

# Notebooks funktionieren automatisch mit robuster Import-LÃ¶sung
```

### 2. Umgebung konfigurieren

```bash
# API-SchlÃ¼ssel setzen
export OPENAI_API_KEY="your-api-key-here"
```

### 3. Jupyter Notebook starten

```bash
# Notebook Ã¶ffnen
jupyter notebook src/ResearchRAG.ipynb
```

Das Notebook fÃ¼hrt Sie durch:
- Pipeline-Konfiguration
- Dokument-Indexierung
- Query-AusfÃ¼hrung
- Komponenten-Analyse

## ğŸ”§ VerfÃ¼gbare Komponenten

### Baseline-Implementierungen (Phase 1)
- **Chunker**: `LineChunker` - Einfaches zeilenbasiertes Chunking
- **Embedding**: `OpenAIEmbedding` - OpenAI text-embedding-3-small
- **Vector Store**: `InMemoryVectorStore` - In-Memory-Speicher mit sklearn
- **Language Model**: `OpenAILanguageModel` - OpenAI GPT-4o-mini

### Geplante Erweiterungen (Phase 2-4)
- **Chunker**: `RecursiveChunker`, `SemanticChunker`
- **Embedding**: `SentenceTransformerEmbedding`, `HuggingFaceEmbedding`
- **Vector Store**: `ChromaVectorStore`, `FAISSVectorStore`
- **Language Model**: `OllamaLanguageModel`, `HuggingFaceLanguageModel`

## ğŸ“Š Beispiel-Konfiguration

```python
from src.config.pipeline_configs import get_baseline_config
from src.core.rag_pipeline import RAGPipeline

# Baseline-Konfiguration laden
config = get_baseline_config()

# Pipeline erstellen
pipeline = RAGPipeline(config)

# Dokumente indexieren
documents = pipeline.load_documents_from_file("data/raw/dsgvo.txt")
pipeline.index_documents(documents)

# Query ausfÃ¼hren
result = pipeline.query("Was ist die maximale GeldbuÃŸe nach Art. 83 DSGVO?")
print(result['answer'])
```

## ğŸ”¬ Experimentieren

### Neue Komponente hinzufÃ¼gen

1. **Basisklasse implementieren**:
```python
from src.components.chunkers import BaseChunker

class MyCustomChunker(BaseChunker):
    def chunk_text(self, text: str) -> List[str]:
        # Ihre Implementierung hier
        pass
```

2. **Komponente registrieren**:
```python
from src.core.component_loader import component_loader
component_loader.register_chunker("my_custom", MyCustomChunker)
```

3. **Konfiguration erstellen**:
```python
from src.config.pipeline_configs import create_custom_config

config = create_custom_config(
    chunker_type="my_custom",
    embedding_type="openai",
    vector_store_type="in_memory",
    llm_type="openai"
)
```

### Konfigurationen vergleichen

```python
from src.config.pipeline_configs import get_alternative_configs

configs = get_alternative_configs()
for name, config in configs.items():
    print(f"{name}: {config.get_component_types()}")
```

## ğŸ“ˆ Evaluierung

### QA-Datensatz
Das Projekt enthÃ¤lt einen DSGVO-spezifischen QA-Datensatz mit verschiedenen Fragetypen:
- **Faktenfragen**: "Was ist die maximale GeldbuÃŸe?"
- **Prozessfragen**: "Wie lÃ¤uft eine Datenschutz-FolgenabschÃ¤tzung ab?"
- **ProblemlÃ¶sungsfragen**: "Was ist bei Marketing-Nutzung zu beachten?"

### Metriken (geplant)
- **Retrieval**: Precision@k, Recall@k, F1@k, MRR, NDCG
- **Generation**: ROUGE-L, BLEU, BERTScore, Semantic Similarity
- **RAG**: RAGAS, Faithfulness, Groundedness
- **Performance**: Inferenzzeit, Speicherverbrauch, Durchsatz

## ğŸ› ï¸ Entwicklung

### Projektstruktur erweitern

```bash
# Neue Komponente hinzufÃ¼gen
touch src/components/chunkers/my_new_chunker.py

# Tests schreiben
touch tests/unit/test_my_new_chunker.py

# Dokumentation aktualisieren
echo "## MyNewChunker" >> docs/components.md
```

### Code-QualitÃ¤t

```bash
# Tests ausfÃ¼hren
pytest tests/

# Code formatieren
black src/

# Linting
flake8 src/
```

## ğŸ“š FÃ¼r Google Colab

Das System ist fÃ¼r Google Colab optimiert:

1. **Automatische Package-Installation**
2. **Google Drive-Integration** fÃ¼r Persistierung
3. **GPU-UnterstÃ¼tzung** fÃ¼r Embeddings
4. **Memory-Management** fÃ¼r groÃŸe Dokumente

```python
# In Google Colab
!git clone <repository-url>
%cd FOM.BigDataAnalyseProjekt
!pip install -r requirements.txt

# Google Drive mounten
from google.colab import drive
drive.mount('/content/drive')
```

## ğŸ¤ Beitragen

### FÃ¼r Teammitglieder

1. **Fork** das Repository
2. **Branch** fÃ¼r Ihr Feature erstellen (`git checkout -b feature/chunking-strategy`)
3. **Commit** Ihre Ã„nderungen (`git commit -am 'Add new chunking strategy'`)
4. **Push** zum Branch (`git push origin feature/chunking-strategy`)
5. **Pull Request** erstellen

### Coding-Standards

- **Docstrings**: Alle Ã¶ffentlichen Funktionen dokumentieren
- **Type Hints**: Verwenden Sie Type Annotations
- **Tests**: Mindestens 80% Code Coverage
- **Logging**: Verwenden Sie das logging-Modul

## ğŸ“„ Lizenz

Dieses Projekt ist fÃ¼r Bildungszwecke an der FOM entwickelt.

## ğŸ‘¥ Team

- **Person A**: Chunking-Strategien
- **Person B**: Embedding-Verfahren
- **Person C**: Vector Stores & Retrieval
- **Person D**: Language Models & Generation

## ğŸ”— NÃ¼tzliche Links

- [OpenAI API Documentation](https://platform.openai.com/docs)
- [Sentence Transformers](https://www.sbert.net/)
- [ChromaDB Documentation](https://docs.trychroma.com/)
- [FAISS Documentation](https://faiss.ai/)
- [RAGAS Evaluation Framework](https://docs.ragas.io/)

---

**Status**: Phase 1 abgeschlossen (Baseline-System funktionsfÃ¤hig)
**NÃ¤chste Schritte**: Implementierung erweiterte Komponenten (Phase 2-4)