# Simple Research RAG ğŸ”

Ein einfaches, modulares RAG-System (Retrieval-Augmented Generation) zur Evaluierung verschiedener Sprachmodelle, mit besonderem Fokus auf deutsche und On-Premise-Modelle.

## ğŸ¯ Projektziel

Dieses Projekt ermÃ¶glicht die systematische Evaluierung und den Vergleich verschiedener Sprachmodelle fÃ¼r deutsche Dokumente durch ein benutzerfreundliches RAG-System.

## âœ¨ Features

- **Modulare Architektur**: Einfacher Wechsel zwischen verschiedenen Sprachmodellen
- **Mehrere Model-Provider**: OpenAI, Groq, Ollama (lokal)
- **Umfassende Evaluierung**: RAGAS, BERTScore, Performance-Metriken
- **Web-Interface**: Streamlit-basierte BenutzeroberflÃ¤che
- **Deutsche Optimierung**: Spezielle UnterstÃ¼tzung fÃ¼r deutsche Texte

## ğŸ—ï¸ Architektur

```
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_loader.py      # Dokumenten-Loading und Chunking
â”‚   â”œâ”€â”€ retriever.py        # FAISS-basierte Vektorsuche
â”‚   â”œâ”€â”€ generator.py        # LLM-Provider (OpenAI, Groq, Ollama)
â”‚   â”œâ”€â”€ evaluator.py        # Evaluierungsmetriken
â”‚   â””â”€â”€ main.py            # Hauptorchestrator
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ documents/          # Dokumente fÃ¼r RAG
â”‚   â””â”€â”€ test_questions.json # Testfragen mit Referenzantworten
â”œâ”€â”€ results/               # Evaluierungsergebnisse
â”œâ”€â”€ config.yaml           # Systemkonfiguration
â””â”€â”€ app.py                # Streamlit Web-Interface
```

## ğŸš€ Installation

### 1. Repository klonen
```bash
git clone <repository-url>
cd ResearchRAG
```

### 2. Virtuelle Umgebung erstellen
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# oder
venv\Scripts\activate     # Windows
```

### 3. Dependencies installieren
```bash
pip install -r requirements.txt
```

### 4. Umgebungsvariablen setzen
```bash
# .env Datei erstellen oder direkt setzen
export OPENAI_API_KEY="your-openai-api-key"
export GROQ_API_KEY="your-groq-api-key"
```

### 5. Lokale Modelle installieren (optional)
FÃ¼r Ollama-Modelle:
```bash
# Ollama installieren: https://ollama.ai/
ollama pull llama3.1:8b-sauerkraut
ollama pull qwen2.5:7b-instruct
```

## ğŸ“– Nutzung

### Command Line Interface

#### 1. System Setup
```bash
# Daten laden und Index erstellen
python src/main.py --setup

# Index neu erstellen (force)
python src/main.py --rebuild
```

#### 2. Einzelne Anfrage
```bash
python src/main.py --query "Was ist die DSGVO?"
```

#### 3. Modell wechseln
```bash
# VerfÃ¼gbare Modelle anzeigen
python src/main.py --list-models

# Zu anderem Modell wechseln
python src/main.py --model gpt-4o-mini
python src/main.py --model mixtral-8x7b
python src/main.py --model llama-sauerkraut
```

#### 4. Evaluierung ausfÃ¼hren
```bash
# VollstÃ¤ndige Evaluierung
python src/main.py --evaluate

# Begrenzte TestgrÃ¶ÃŸe
python src/main.py --evaluate --test-size 10
```

#### 5. Modelle vergleichen
```bash
python src/main.py --compare
```

### Web Interface

```bash
streamlit run app.py
```

Das Web-Interface bietet:
- **Query Tab**: Interaktive Fragestellung mit Kontextanzeige
- **Evaluation Tab**: Modell-Evaluierung mit Metriken
- **Results Analysis**: Vergleich verschiedener Modelle
- **Settings**: Systemkonfiguration und Diagnostik

## ğŸ”§ Konfiguration

Die `config.yaml` Datei steuert alle Systemparameter:

```yaml
# Modell-Konfiguration
model:
  name: "gpt-4o-mini"
  provider: "openai"
  temperature: 0.1
  max_tokens: 1000

# Retrieval-Konfiguration
retrieval:
  embedding_model: "text-embedding-3-small"
  chunk_size: 500
  chunk_overlap: 100
  top_k: 5

# Evaluierungs-Konfiguration
evaluation:
  metrics: ["ragas", "bertscore", "performance"]
  test_size: 50
```

## ğŸ¤– UnterstÃ¼tzte Modelle

| Modell | Provider | Beschreibung |
|--------|----------|--------------|
| `gpt-4o-mini` | OpenAI | Schnell und kostengÃ¼nstig |
| `mixtral-8x7b` | Groq | Mixtral 8x7B via Groq API |
| `llama-sauerkraut` | Ollama | Deutsch-optimiertes Llama |
| `qwen2.5-7b` | Ollama | Qwen 2.5 7B Instruct |

## ğŸ“Š Evaluierungsmetriken

### RAGAS Metriken
- **Answer Relevancy**: Relevanz der Antwort zur Frage
- **Faithfulness**: Treue zum Kontext
- **Context Relevancy**: Relevanz des abgerufenen Kontexts

### BERTScore
- **Precision, Recall, F1**: Semantische Ã„hnlichkeit mit Referenzantworten

### Performance Metriken
- **Execution Time**: Antwortzeit
- **Tokens/Second**: Durchsatz
- **Cost Estimate**: GeschÃ¤tzte Kosten (API-Modelle)

## ğŸ“ Datenstruktur

### Dokumente hinzufÃ¼gen
Legen Sie Textdateien (.txt, .md) in `data/documents/` ab:
```
data/documents/
â”œâ”€â”€ dsgvo_beispiel.txt
â”œâ”€â”€ weitere_dokumente.txt
â””â”€â”€ ...
```

### Testfragen erstellen
Format fÃ¼r `data/test_questions.json`:
```json
[
  {
    "id": 1,
    "question": "Was ist die DSGVO?",
    "reference_answer": "Die DSGVO ist eine EU-Verordnung...",
    "category": "basics"
  }
]
```

## ğŸ” Beispiel-Workflow

1. **Setup**:
   ```bash
   python src/main.py --setup
   ```

2. **Baseline erstellen**:
   ```bash
   python src/main.py --model gpt-4o-mini
   python src/main.py --evaluate --test-size 20
   ```

3. **Alternativen testen**:
   ```bash
   python src/main.py --model mixtral-8x7b
   python src/main.py --evaluate --test-size 20

   python src/main.py --model llama-sauerkraut
   python src/main.py --evaluate --test-size 20
   ```

4. **Ergebnisse vergleichen**:
   ```bash
   python src/main.py --compare
   ```

## ğŸ“ˆ Ergebnisse

Evaluierungsergebnisse werden in `results/` gespeichert:
- `{model}_detailed_{timestamp}.json`: Detailierte Ergebnisse
- `{model}_aggregates_{timestamp}.json`: Zusammengefasste Metriken
- `{model}_results_{timestamp}.csv`: CSV fÃ¼r weitere Analyse
- `model_comparison.csv`: Modellvergleich

## ğŸ”§ Troubleshooting

### HÃ¤ufige Probleme

1. **API-SchlÃ¼ssel fehlen**:
   ```bash
   export OPENAI_API_KEY="your-key"
   export GROQ_API_KEY="your-key"
   ```

2. **Ollama nicht erreichbar**:
   ```bash
   ollama serve  # Ollama-Server starten
   ```

3. **Dependencies fehlen**:
   ```bash
   pip install -r requirements.txt
   ```

4. **Speicher-Probleme bei FAISS**:
   - Chunk-GrÃ¶ÃŸe reduzieren in `config.yaml`
   - Weniger Dokumente verwenden

### Diagnostik ausfÃ¼hren
```bash
# CLI
python src/main.py --help

# Web-Interface
# Gehen Sie zu Settings > System Diagnostics
```

## ğŸ¤ Entwicklung

### Neue Modelle hinzufÃ¼gen

1. **Generator erweitern** (`src/generator.py`):
   ```python
   class NewModelGenerator(BaseGenerator):
       def generate(self, query, context):
           # Implementation
   ```

2. **Factory-Funktion aktualisieren**:
   ```python
   def get_generator(config):
       if config["provider"] == "new_provider":
           return NewModelGenerator(config)
   ```

3. **Modell zu `AVAILABLE_MODELS` hinzufÃ¼gen**

### Neue Metriken hinzufÃ¼gen

1. **Evaluator erweitern** (`src/evaluator.py`):
   ```python
   def _evaluate_custom_metric(self, generated, reference):
       # Implementation
   ```

2. **Metrik in `evaluate_single` integrieren**

## ğŸ“ Lizenz

MIT License - siehe LICENSE Datei fÃ¼r Details.

## ğŸ™‹â€â™‚ï¸ Support

Bei Fragen oder Problemen:
1. ÃœberprÃ¼fen Sie die Troubleshooting-Sektion
2. FÃ¼hren Sie die Systemdiagnose aus
3. Ã–ffnen Sie ein Issue im Repository

## ğŸ”® Roadmap

- [ ] Weitere Embedding-Modelle
- [ ] Advanced Chunking-Strategien
- [ ] Multi-Modal RAG
- [ ] Erweiterte Evaluierungsmetriken
- [ ] Docker-Container
- [ ] Cloud-Deployment

---

**Happy RAG-ing! ğŸš€**