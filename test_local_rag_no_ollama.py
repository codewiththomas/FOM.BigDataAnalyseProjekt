#!/usr/bin/env python3
"""
Test-Skript fÃ¼r lokales ResearchRAG-System ohne Ollama
Testet nur die lokalen Komponenten (Chunking, Embedding, Vector Store)
"""

import sys
import os
import json
from pathlib import Path

# Pfad zum src-Verzeichnis hinzufÃ¼gen
sys.path.insert(0, 'src')

def test_local_rag_no_ollama():
    print("ğŸš€ Teste lokales ResearchRAG-System (ohne Ollama)...")

    try:
        # Imports
        from config.pipeline_configs import get_local_config
        from core.rag_pipeline import RAGPipeline

        print("âœ… Module erfolgreich importiert")

        # Lokale Konfiguration laden und fÃ¼r OpenAI-Fallback anpassen
        config = get_local_config()

        # OpenAI als Fallback fÃ¼r LLM verwenden
        config_dict = config._config.copy()
        config_dict["language_model"] = {
            "type": "openai",
            "model": "gpt-4o-mini",
            "temperature": 0.1,
            "max_tokens": 500
        }

        from config.pipeline_configs import PipelineConfig
        config = PipelineConfig(config_dict)

        print(f"âœ… Konfiguration geladen: {config.get_component_types()}")

        # Pipeline erstellen
        print("ğŸ”§ Erstelle Pipeline...")
        pipeline = RAGPipeline(config)
        print("âœ… Pipeline erstellt")

        # Test-Dokument laden
        print("ğŸ“„ Lade DSGVO-Dokument...")
        dsgvo_path = Path("data/raw/dsgvo.txt")
        if not dsgvo_path.exists():
            print(f"âŒ DSGVO-Datei nicht gefunden: {dsgvo_path}")
            return False

        with open(dsgvo_path, 'r', encoding='utf-8') as f:
            dsgvo_text = f.read()

        print(f"âœ… Dokument geladen: {len(dsgvo_text)} Zeichen")

        # Dokument indexieren
        print("ğŸ” Indexiere Dokument...")
        pipeline.index_documents([dsgvo_text])
        print("âœ… Dokument indexiert")

        # Test-Fragen
        test_questions = [
            "Was ist die maximale GeldbuÃŸe nach der DSGVO?",
            "Welche Rechte haben betroffene Personen?",
            "Was ist eine Datenschutz-FolgenabschÃ¤tzung?"
        ]

        print("\nğŸ§ª Teste Retrieval (ohne Generation)...")

        # Nur Retrieval testen (ohne LLM)
        for i, question in enumerate(test_questions, 1):
            print(f"\n--- Frage {i}: {question} ---")

            # Embedding fÃ¼r Frage erstellen
            question_embedding = pipeline.embedding.embed_texts([question])

            # Ã„hnliche Chunks finden
            similar_chunks = pipeline.vector_store.similarity_search(
                question_embedding[0], top_k=3
            )

            print(f"âœ… {len(similar_chunks)} relevante Chunks gefunden")

            # Erste 2 Chunks anzeigen
            for j, chunk in enumerate(similar_chunks[:2]):
                chunk_text = chunk['text'][:200] + "..." if len(chunk['text']) > 200 else chunk['text']
                print(f"  ğŸ“ Chunk {j+1}: {chunk_text}")
                print(f"     Ã„hnlichkeit: {chunk.get('score', 'N/A'):.3f}")

        print("\nâœ… Lokales RAG-System funktioniert!")
        print("\nğŸ“Š System-Status:")
        print(f"  - Chunker: {config.get_component_types()['chunker']}")
        print(f"  - Embedding: {config.get_component_types()['embedding']}")
        print(f"  - Vector Store: {config.get_component_types()['vector_store']}")
        print(f"  - Chunks im Index: {len(pipeline.vector_store.texts) if hasattr(pipeline.vector_store, 'texts') else 'N/A'}")

        return True

    except Exception as e:
        print(f"âŒ Fehler: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_local_rag_no_ollama()
    if success:
        print("\nğŸ‰ Test erfolgreich abgeschlossen!")
        print("\nNÃ¤chste Schritte:")
        print("1. Installiere Ollama fÃ¼r vollstÃ¤ndige lokale Nutzung")
        print("2. Oder nutze OpenAI API fÃ¼r Generation")
        print("3. Erweitere QA-Datensatz in data/evaluation/qa_pairs.json")
    else:
        print("\nâŒ Test fehlgeschlagen!")
        sys.exit(1)