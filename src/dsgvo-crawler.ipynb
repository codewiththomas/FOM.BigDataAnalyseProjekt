{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9e13805",
   "metadata": {},
   "source": [
    "# Crawler für die DSGVO-Website\n",
    "\n",
    "Die DSGVO kann in aufbereiteter Form auf der Webseite https://dsgvo-gesetz.de/ eingesehen werden. Hierbei handelt es sich um eine von intersoft consulting erstellte Webseite, die die DSGVO in einem einfachen Format präsentiert.\n",
    "\n",
    "## Grundaufbau der Seiten\n",
    "\n",
    "Das Grundgerüst aller Seiten folgt dem folgenden Schema:\n",
    "\n",
    "```html\n",
    "...\n",
    "<body ...>\n",
    "  <div id=\"page\" ...>\n",
    "    <div id=\"content\" ...>\n",
    "      <div id=\"primary\" ...>\n",
    "        <main id=\"main\" ...>\n",
    "          <article id=\"post-##\" ...>\n",
    "            <header class=\"entry-header\">\n",
    "              <h1 class=\"entry-title\">\n",
    "                <span class=\"dsgvo-number\">[TITEL]</span>\n",
    "                <span class=\"dsgvo-title\">[UNTERTITEL]</span>\n",
    "              </h1>\n",
    "            </header> \n",
    "            <div class=\"entry-content\">\n",
    "              [INHALT]\n",
    "            </div>  \n",
    "          </article>\n",
    "        </main>\n",
    "      </div>\n",
    "    </div>\n",
    "  </div>\n",
    "</body>\n",
    "...\n",
    "```\n",
    "\n",
    "## Aufbau der Übersichtsseite\n",
    "\n",
    "Die Übersichtsseite enthält einen kurzen erklärenden Text und danach Links zu den 11 Unterkapiteln sowie direkte Links zu den einzelnen Artikeln.\n",
    "\n",
    "Der Titel der Seite ist \"Datenschutz-Grundverordnung\" und der Subtitel \"DSGVO\".\n",
    "\n",
    "Die URL lautet: https://dsgvo-gesetz.de/\n",
    "\n",
    "Der folgende Code zeigt den Inhalt der Seite, also des Platzhalters `[INHALT]`:\n",
    "\n",
    "```html\n",
    "::before\n",
    "<p>...</p>\n",
    "<h2>Schnellzugriff</h2>\n",
    "<table ...>...</table>\n",
    "<h2>Wichtige Themen</h2>\n",
    "<ul class=\"beliebte inhalte\">...</ul>\n",
    "<div class=\"clear\">...</div>\n",
    "<h2>Inhaltsverzeichnis</h2>\n",
    "<div class=\"liste-inhaltsuebersicht dsgvo\">\n",
    "  [VERZEICHNIS]\n",
    "</div>\n",
    "<div class=\"feedback hidden-print\">...</div>\n",
    "::after\n",
    "```\n",
    "\n",
    "Das Verzeichnis selbst ist wie folgt aufgebaut (Beispiel):\n",
    "\n",
    "```html\n",
    "<div class=\"kapitel nomargin\">...</div>\n",
    "<div class=\"artikel\">...</div>\n",
    "<div class=\"kapitel\">...</div>\n",
    "<div class=\"abschnitt\">...</div>\n",
    "<div class=\"artikel\">...</div>\n",
    "<div class=\"artikel\">...</div>\n",
    "<div class=\"abschnitt\">...</div>\n",
    "<div class=\"artikel\">...</div>\n",
    "<div class=\"artikel\">...</div>\n",
    "```\n",
    "\n",
    "Dabei ist zu beachten, dass einzelne Kapitel entweder nur aus Artikeln bestehen (also keine Abschnitte enthalten), oder aus Abschnitten und Artikeln.\n",
    "\n",
    "```html\n",
    "<div class=\"kapitel\">\n",
    "  <span class=\"nummer\">Kapitel 2</span>\n",
    "  <span class=\"titel\">Grundsätze</span>\n",
    "</div>\n",
    "```\n",
    "\n",
    "```html\n",
    "<div class=\"abschnitt\">\n",
    "  <span class=\"nummer\">Abschnitt 1</span>\n",
    "  <span class=\"titel\">Transparenz und Modalitäten</span>\n",
    "</div>\n",
    "```\n",
    "\n",
    "```html\n",
    "<div class=\"artikel\">\n",
    "  <a href=\"https://dsgvo-gesetz.de/art-2-dsgvo/\">\n",
    "    <span class=\"nummer\">Artikel 2</span>\n",
    "    <span class=\"titel\">Sachlicher Anwendungsbereich</span>\n",
    "  </a>\n",
    "</div>\n",
    "```\n",
    "\n",
    "Beim Crawlen der Seiten wird das Verzeichnis in der Übersichtsseite durchlaufen und die Links zu den einzelnen Artikeln ausgelesen. Dabei ist die Reihenfolge entscheidend, da durch diese klar wird, zu welchem Kapitel und ggf. Abschnitt ein Artikel gehört.\n",
    "\n",
    "## Aufbau einer Artikelseite\n",
    "\n",
    "URL eines Artikels: https://dsgvo-gesetz.de/art-##-dsgvo/\n",
    "\n",
    "Der Abschnitt [INHALT] ist im einfachsten Fall wie folgt aufgaubaut, wie bspw. Artikel 10:\n",
    "\n",
    "```html\n",
    "<p>[TEXT]</p>\n",
    "```\n",
    "\n",
    "Meistens besteht ein Artikel aber aus mehreren Absätzen, welche als `(n)` nummeriert sind. Ein Beispiel dafür wäre Artikel 1. In diesem Fall wäre die Darstellung wie folgt:\n",
    "\n",
    "```html\n",
    "<ol>\n",
    "  <li>[TEXT_ABSATZ_1]</li>\n",
    "  <li>[TEXT_ABSATZ_2]</li>\n",
    "  <li>[TEXT_ABSATZ_3]</li>\n",
    "</ol>\n",
    "```\n",
    "\n",
    "Es ist ausßerdem möglich, dass ein Absatz in mehrere Unterabsätze unterteilt ist. Ein Beispiel wäre Artikel 2:\n",
    "\n",
    "```html\t\n",
    "<ol>\n",
    "  <li>[TEXT_ABSATZ_1]</li>\n",
    "  <li>\n",
    "    <p>[TEXT_UNTERABSATZ_0</p>\n",
    "    <ol>\n",
    "      <li>[TEXT_UNTERABSATZ_1]</li>\n",
    "      <li>[TEXT_UNTERABSATZ_2]</li>\n",
    "      <li>[TEXT_UNTERABSATZ_3]</li>\n",
    "      <li>[TEXT_UNTERABSATZ_4]</li>\n",
    "    <ol>  \n",
    "  </li>\n",
    "  <li>[TEXT_ABSATZ_3]</li>\n",
    "  <li>[TEXT_ABSATZ_4]</li>\n",
    "</ol>\n",
    "```\n",
    "\n",
    "Jeder der [TEXT...]-Platzhalter kann aus mehreren Sätzen bestehen. Falls dem so ist, wird jedem Satz ein `<sup>#</sup>` vorangestellt. Sollte nur ein Satz vorhanden sein, wird dieser ohne `<sup>#</sup>` dargestellt und es ist von Satz 1 auszugehen.\n",
    "\n",
    "## Zielformat\n",
    "\n",
    "Es soll eine JSONL-Datei erstellt werden, welche die Daten in einem strukturierten Format enthält:\n",
    "- Kapitel_Nr\n",
    "- Kapitel_Name\n",
    "- Abschnitt_Nr\n",
    "- Abschnitt_Name\n",
    "- Artikel_Nr\n",
    "- Artikel_Name\n",
    "- Absatz_Nr (0 falls nicht explizit durch (#) angegeben)\n",
    "- Unterabsatz_Nr (0 falls nicht explizit durch (a), (b), (c), ... angegeben, ansonsten a=1, b=2, c=3, ...)\n",
    "- Satz_Nr (1 falls nicht explizit durch <sup>#</sup> angegeben)\n",
    "- Text\n",
    "\n",
    "## Ausgabe\n",
    "\n",
    "Vom /src-Ordner ausgesehen:\n",
    "../data/input/dsgvo_crawled.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b08f37cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict\n",
    "\n",
    "@dataclass\n",
    "class DSGVOEntry:\n",
    "    \"\"\"Repräsentiert einen DSGVO-Eintrag mit hierarchischer Struktur\"\"\"\n",
    "    kapitel_nr: int\n",
    "    kapitel_name: str\n",
    "    abschnitt_nr: int       # 0 falls nicht vorhanden\n",
    "    abschnitt_name: str     # \"\" falls nicht vorhanden\n",
    "    artikel_nr: int\n",
    "    artikel_name: str\n",
    "    absatz_nr: int          # 0 falls nicht explizit durch (#) angegeben\n",
    "    unterabsatz_nr: int     # 0 falls nicht explizit durch (a), (b), (c), ... angegeben, ansonsten a=1, b=2, c=3, ...\n",
    "    satz_nr: int            # 1 falls nicht explizit durch <sup>#</sup> angegeben\n",
    "    text: str\n",
    "\n",
    "    def to_dict(self) -> Dict:\n",
    "        \"\"\"Konvertiert den Eintrag zu einem Dictionary für JSONL-Export\"\"\"\n",
    "        return {\n",
    "            'Kapitel_Nr': self.kapitel_nr,\n",
    "            'Kapitel_Name': self.kapitel_name,\n",
    "            'Abschnitt_Nr': self.abschnitt_nr,\n",
    "            'Abschnitt_Name': self.abschnitt_name,\n",
    "            'Artikel_nr': self.artikel_nr,\n",
    "            'Artikel_Name': self.artikel_name,\n",
    "            'Absatz_nr': self.absatz_nr,\n",
    "            'Unterabsatz_nr': self.unterabsatz_nr,\n",
    "            'Satz_nr': self.satz_nr,\n",
    "            'Text': self.text\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a21ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List, Tuple, Optional\n",
    "import re\n",
    "\n",
    "# Logging Setup\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('../data/logs/crawler.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3ddb64d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DSGVOCrawler:\n",
    "    \"\"\"DSGVO Website Crawler für https://dsgvo-gesetz.de/\"\"\"\n",
    "\n",
    "    def __init__(self, base_url: str = \"https://dsgvo-gesetz.de\"):\n",
    "        self.base_url = base_url\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        })\n",
    "\n",
    "        # Struktur-Tracking\n",
    "        self.current_kapitel = {\"nr\": 0, \"name\": \"\"}\n",
    "        self.current_abschnitt = {\"nr\": 0, \"name\": \"\"}\n",
    "\n",
    "    def get_page(self, url: str, retry_count: int = 3) -> Optional[BeautifulSoup]:\n",
    "        \"\"\"Lädt eine Seite mit Retry-Mechanismus\"\"\"\n",
    "        for attempt in range(retry_count):\n",
    "            try:\n",
    "                logger.info(f\"Lade Seite: {url} (Versuch {attempt + 1})\")\n",
    "                response = self.session.get(url, timeout=10)\n",
    "                response.raise_for_status()\n",
    "\n",
    "                soup = BeautifulSoup(response.content, 'lxml')\n",
    "                time.sleep(1)  # Rate limiting\n",
    "                return soup\n",
    "\n",
    "            except requests.RequestException as e:\n",
    "                logger.warning(f\"Fehler beim Laden von {url}: {e}\")\n",
    "                if attempt == retry_count - 1:\n",
    "                    logger.error(f\"Alle Versuche fehlgeschlagen für: {url}\")\n",
    "                    return None\n",
    "                time.sleep(2 ** attempt)  # Exponential backoff\n",
    "\n",
    "        return None\n",
    "\n",
    "    def parse_overview_page(self) -> List[Tuple[str, dict]]:\n",
    "        \"\"\"\n",
    "        Parst die Übersichtsseite und extrahiert alle Artikel-URLs\n",
    "        Returns: Liste von (artikel_url, artikel_context) Tupeln\n",
    "        \"\"\"\n",
    "        soup = self.get_page(self.base_url)\n",
    "        if not soup:\n",
    "            logger.error(\"Konnte Übersichtsseite nicht laden\")\n",
    "            return []\n",
    "\n",
    "        # Finde das Inhaltsverzeichnis\n",
    "        inhaltsverzeichnis = soup.find('div', class_='liste-inhaltsuebersicht dsgvo')\n",
    "        if not inhaltsverzeichnis:\n",
    "            logger.error(\"Inhaltsverzeichnis nicht gefunden\")\n",
    "            return []\n",
    "\n",
    "        artikel_links = []\n",
    "\n",
    "        # Durchlaufe alle Elemente im Verzeichnis\n",
    "        for element in inhaltsverzeichnis.find_all(['div']):\n",
    "            class_name = element.get('class', [])\n",
    "\n",
    "            if 'kapitel' in class_name:\n",
    "                # Neues Kapitel\n",
    "                nummer_span = element.find('span', class_='nummer')\n",
    "                titel_span = element.find('span', class_='titel')\n",
    "\n",
    "                if nummer_span and titel_span:\n",
    "                    # Extrahiere Kapitelnummer\n",
    "                    kapitel_text = nummer_span.get_text(strip=True)\n",
    "                    kapitel_match = re.search(r'(\\d+)', kapitel_text)\n",
    "                    self.current_kapitel['nr'] = int(kapitel_match.group(1)) if kapitel_match else 0\n",
    "                    self.current_kapitel['name'] = titel_span.get_text(strip=True)\n",
    "\n",
    "                    # Reset Abschnitt\n",
    "                    self.current_abschnitt = {\"nr\": 0, \"name\": \"\"}\n",
    "\n",
    "                    logger.info(f\"Gefunden: Kapitel {self.current_kapitel['nr']} - {self.current_kapitel['name']}\")\n",
    "\n",
    "            elif 'abschnitt' in class_name:\n",
    "                # Neuer Abschnitt\n",
    "                nummer_span = element.find('span', class_='nummer')\n",
    "                titel_span = element.find('span', class_='titel')\n",
    "\n",
    "                if nummer_span and titel_span:\n",
    "                    # Extrahiere Abschnittsnummer\n",
    "                    abschnitt_text = nummer_span.get_text(strip=True)\n",
    "                    abschnitt_match = re.search(r'(\\d+)', abschnitt_text)\n",
    "                    self.current_abschnitt['nr'] = int(abschnitt_match.group(1)) if abschnitt_match else 0\n",
    "                    self.current_abschnitt['name'] = titel_span.get_text(strip=True)\n",
    "\n",
    "                    logger.info(f\"Gefunden: Abschnitt {self.current_abschnitt['nr']} - {self.current_abschnitt['name']}\")\n",
    "\n",
    "            elif 'artikel' in class_name:\n",
    "                # Artikel-Link\n",
    "                link = element.find('a')\n",
    "                if link and link.get('href'):\n",
    "                    artikel_url = link.get('href')\n",
    "\n",
    "                    # Kontext für diesen Artikel speichern\n",
    "                    context = {\n",
    "                        'kapitel_nr': self.current_kapitel['nr'],\n",
    "                        'kapitel_name': self.current_kapitel['name'],\n",
    "                        'abschnitt_nr': self.current_abschnitt['nr'],\n",
    "                        'abschnitt_name': self.current_abschnitt['name']\n",
    "                    }\n",
    "\n",
    "                    artikel_links.append((artikel_url, context))\n",
    "\n",
    "        logger.info(f\"Insgesamt {len(artikel_links)} Artikel gefunden\")\n",
    "        return artikel_links\n",
    "\n",
    "    def parse_article_page(self, url: str, context: dict) -> List[DSGVOEntry]:\n",
    "        \"\"\"\n",
    "        Parst eine einzelne Artikelseite und extrahiert alle Einträge\n",
    "        \"\"\"\n",
    "        soup = self.get_page(url)\n",
    "        if not soup:\n",
    "            logger.error(f\"Konnte Artikel nicht laden: {url}\")\n",
    "            return []\n",
    "\n",
    "        entries = []\n",
    "\n",
    "        # Extrahiere Artikel-Info aus dem Header\n",
    "        header = soup.find('header', class_='entry-header')\n",
    "        if not header:\n",
    "            logger.error(f\"Header nicht gefunden in: {url}\")\n",
    "            return []\n",
    "\n",
    "        # Artikel Nummer und Name\n",
    "        dsgvo_number = header.find('span', class_='dsgvo-number')\n",
    "        dsgvo_title = header.find('span', class_='dsgvo-title')\n",
    "\n",
    "        if not dsgvo_number or not dsgvo_title:\n",
    "            logger.error(f\"Artikel-Info nicht gefunden in: {url}\")\n",
    "            return []\n",
    "\n",
    "        artikel_text = dsgvo_number.get_text(strip=True)\n",
    "        artikel_match = re.search(r'(\\d+)', artikel_text)\n",
    "        artikel_nr = int(artikel_match.group(1)) if artikel_match else 0\n",
    "        artikel_name = dsgvo_title.get_text(strip=True)\n",
    "\n",
    "        logger.info(f\"Parse Artikel {artikel_nr}: {artikel_name}\")\n",
    "\n",
    "        # Extrahiere Inhalt\n",
    "        content_div = soup.find('div', class_='entry-content')\n",
    "        if not content_div:\n",
    "            logger.error(f\"Content nicht gefunden in: {url}\")\n",
    "            return []\n",
    "\n",
    "        # Prüfe ob es sich um einen einfachen Artikel handelt (nur <p> Tags)\n",
    "        paragraphs = content_div.find_all('p', recursive=False)\n",
    "        if paragraphs and not content_div.find('ol', recursive=False):\n",
    "            # Einfacher Artikel ohne nummerierte Absätze\n",
    "            for p in paragraphs:\n",
    "                text = p.get_text(strip=True)\n",
    "                if text and not self._is_navigation_text(text):\n",
    "                    # Prüfe auf Satz-Nummerierung mit <sup>\n",
    "                    sentences = self._parse_sentences_from_element(p)\n",
    "                    for satz_nr, satz_text in sentences:\n",
    "                        entry = DSGVOEntry(\n",
    "                            kapitel_nr=context['kapitel_nr'],\n",
    "                            kapitel_name=context['kapitel_name'],\n",
    "                            abschnitt_nr=context['abschnitt_nr'],\n",
    "                            abschnitt_name=context['abschnitt_name'],\n",
    "                            artikel_nr=artikel_nr,\n",
    "                            artikel_name=artikel_name,\n",
    "                            absatz_nr=0,  # Kein expliziter Absatz\n",
    "                            unterabsatz_nr=0,  # Kein Unterabsatz\n",
    "                            satz_nr=satz_nr,\n",
    "                            text=satz_text\n",
    "                        )\n",
    "                        entries.append(entry)\n",
    "        else:\n",
    "            # Komplexer Artikel mit nummerierten Absätzen\n",
    "            ol_elements = content_div.find_all('ol', recursive=False)\n",
    "            for ol in ol_elements:\n",
    "                entries.extend(self._parse_ordered_list(ol, context, artikel_nr, artikel_name))\n",
    "\n",
    "        return entries\n",
    "\n",
    "    def _parse_ordered_list(self, ol_element, context: dict, artikel_nr: int, artikel_name: str, parent_absatz: int = 0) -> List[DSGVOEntry]:\n",
    "        \"\"\"\n",
    "        Parst eine <ol> Liste rekursiv für Absätze und Unterabsätze\n",
    "        \"\"\"\n",
    "        entries = []\n",
    "\n",
    "        for i, li in enumerate(ol_element.find_all('li', recursive=False), 1):\n",
    "            try:\n",
    "                absatz_nr = i if parent_absatz == 0 else parent_absatz\n",
    "\n",
    "                # Prüfe ob es verschachtelte Listen gibt (Unterabsätze)\n",
    "                nested_ol = li.find('ol')\n",
    "\n",
    "                if nested_ol:\n",
    "                    # Dieser li hat Unterabsätze\n",
    "                    # Extrahiere Text vor der verschachtelten Liste\n",
    "                    li_copy = li.__copy__()\n",
    "                    nested_ol_copy = li_copy.find('ol')\n",
    "                    if nested_ol_copy:\n",
    "                        nested_ol_copy.decompose()  # Entferne die verschachtelte Liste\n",
    "\n",
    "                    intro_text = li_copy.get_text(strip=True)\n",
    "                    if intro_text:\n",
    "                        # Einleitungstext des Absatzes\n",
    "                        sentences = self._parse_sentences_from_text(intro_text)\n",
    "                        for satz_nr, satz_text in sentences:\n",
    "                            entry = DSGVOEntry(\n",
    "                                kapitel_nr=context['kapitel_nr'],\n",
    "                                kapitel_name=context['kapitel_name'],\n",
    "                                abschnitt_nr=context['abschnitt_nr'],\n",
    "                                abschnitt_name=context['abschnitt_name'],\n",
    "                                artikel_nr=artikel_nr,\n",
    "                                artikel_name=artikel_name,\n",
    "                                absatz_nr=absatz_nr,\n",
    "                                unterabsatz_nr=0,  # Einleitungstext\n",
    "                                satz_nr=satz_nr,\n",
    "                                text=satz_text\n",
    "                            )\n",
    "                            entries.append(entry)\n",
    "\n",
    "                    # Parse Unterabsätze\n",
    "                    for j, sub_li in enumerate(nested_ol.find_all('li', recursive=False), 1):\n",
    "                        text = sub_li.get_text(strip=True)\n",
    "                        if text:\n",
    "                            sentences = self._parse_sentences_from_text(text)\n",
    "                            for satz_nr, satz_text in sentences:\n",
    "                                entry = DSGVOEntry(\n",
    "                                    kapitel_nr=context['kapitel_nr'],\n",
    "                                    kapitel_name=context['kapitel_name'],\n",
    "                                    abschnitt_nr=context['abschnitt_nr'],\n",
    "                                    abschnitt_name=context['abschnitt_name'],\n",
    "                                    artikel_nr=artikel_nr,\n",
    "                                    artikel_name=artikel_name,\n",
    "                                    absatz_nr=absatz_nr,\n",
    "                                    unterabsatz_nr=j,  # a=1, b=2, c=3, ...\n",
    "                                    satz_nr=satz_nr,\n",
    "                                    text=satz_text\n",
    "                                )\n",
    "                                entries.append(entry)\n",
    "                else:\n",
    "                    # Normaler Absatz ohne Unterabsätze\n",
    "                    text = li.get_text(strip=True)\n",
    "                    if text:\n",
    "                        sentences = self._parse_sentences_from_text(text)\n",
    "                        for satz_nr, satz_text in sentences:\n",
    "                            entry = DSGVOEntry(\n",
    "                                kapitel_nr=context['kapitel_nr'],\n",
    "                                kapitel_name=context['kapitel_name'],\n",
    "                                abschnitt_nr=context['abschnitt_nr'],\n",
    "                                abschnitt_name=context['abschnitt_name'],\n",
    "                                artikel_nr=artikel_nr,\n",
    "                                artikel_name=artikel_name,\n",
    "                                absatz_nr=absatz_nr,\n",
    "                                unterabsatz_nr=0,\n",
    "                                satz_nr=satz_nr,\n",
    "                                text=satz_text\n",
    "                            )\n",
    "                            entries.append(entry)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Fehler beim Parsen von Absatz {i} in Artikel {artikel_nr}: {e}\")\n",
    "                continue\n",
    "\n",
    "        return entries\n",
    "\n",
    "    def _parse_sentences_from_text(self, text: str) -> List[Tuple[int, str]]:\n",
    "        \"\"\"KORRIGIERTE Satz-Erkennung für <sup> Tags in Strings\"\"\"\n",
    "        sentences = []\n",
    "\n",
    "        # Prüfe ob der Text <sup> Tags enthält\n",
    "        if '<sup>' in text:\n",
    "            # Verwende Regex um <sup>Nummer</sup> und folgenden Text zu extrahieren\n",
    "            pattern = r'<sup[^>]*>(\\d+)</sup>(.*?)(?=<sup[^>]*>\\d+</sup>|$)'\n",
    "            matches = re.findall(pattern, text, re.DOTALL)\n",
    "\n",
    "            for satz_nr_str, content in matches:\n",
    "                satz_nr = int(satz_nr_str)\n",
    "                # Extrahiere nur den Text, entferne HTML-Tags\n",
    "                # Verwende re.sub um HTML-Tags zu entfernen\n",
    "                satz_text = re.sub(r'<[^>]+>', '', content).strip()\n",
    "                if satz_text:\n",
    "                    sentences.append((satz_nr, satz_text))\n",
    "        else:\n",
    "            # Kein <sup> Tag - gesamter Text ist Satz 1\n",
    "            sentences.append((1, text))\n",
    "\n",
    "        return sentences\n",
    "\n",
    "    def _parse_sentences_from_element(self, element) -> List[Tuple[int, str]]:\n",
    "        \"\"\"Verbesserte Satz-Erkennung für BeautifulSoup Elemente mit <sup> Tags und Zahlen-Satznummerierung\"\"\"\n",
    "        sentences = []\n",
    "\n",
    "        # Konvertiere das Element zu einem String für Regex-Verarbeitung\n",
    "        html_str = str(element)\n",
    "\n",
    "        # Prüfe ob <sup> Tags vorhanden sind\n",
    "        if '<sup>' in html_str:\n",
    "            # Verwende Regex um <sup>Nummer</sup> und folgenden Text zu extrahieren\n",
    "            pattern = r'<sup[^>]*>(\\d+)</sup>(.*?)(?=<sup[^>]*>\\d+</sup>|$)'\n",
    "            matches = re.findall(pattern, html_str, re.DOTALL)\n",
    "\n",
    "            for satz_nr_str, content in matches:\n",
    "                satz_nr = int(satz_nr_str)\n",
    "                # Extrahiere nur den Text, entferne HTML-Tags\n",
    "                satz_text = re.sub(r'<[^>]+>', '', content).strip()\n",
    "                if satz_text:\n",
    "                    sentences.append((satz_nr, satz_text))\n",
    "        else:\n",
    "            # Kein <sup> Tag - prüfe auf Zahlen-Satznummerierung (wie \"1Text...2Text...\")\n",
    "            clean_text = re.sub(r'<[^>]+>', '', html_str).strip()\n",
    "            if clean_text:\n",
    "                # Suche nach Mustern wie \"1Text...2Text...\" (Zahlen am Anfang von Sätzen)\n",
    "                # Das Muster sucht nach einer Zahl am Anfang, gefolgt von Text bis zur nächsten Zahl\n",
    "                pattern = r'(\\d+)([^0-9]+?)(?=\\d+|$)'\n",
    "                matches = re.findall(pattern, clean_text, re.DOTALL)\n",
    "\n",
    "                if len(matches) > 1:\n",
    "                    # Mehrere Sätze gefunden\n",
    "                    for satz_nr_str, content in matches:\n",
    "                        satz_nr = int(satz_nr_str)\n",
    "                        satz_text = content.strip()\n",
    "                        if satz_text:\n",
    "                            sentences.append((satz_nr, satz_text))\n",
    "                else:\n",
    "                    # Keine Satznummerierung gefunden - gesamter Text ist Satz 1\n",
    "                    sentences.append((1, clean_text))\n",
    "\n",
    "        return sentences\n",
    "\n",
    "    def _is_navigation_text(self, text: str) -> bool:\n",
    "        \"\"\"\n",
    "        Prüft ob es sich um Navigations- oder Meta-Text handelt\n",
    "        \"\"\"\n",
    "        navigation_keywords = [\n",
    "            'feedback', 'bewertung', 'drucken', 'teilen',\n",
    "            'weitere artikel', 'siehe auch', 'navigation'\n",
    "        ]\n",
    "        text_lower = text.lower()\n",
    "        return any(keyword in text_lower for keyword in navigation_keywords)\n",
    "\n",
    "    def crawl_all_articles(self) -> List[DSGVOEntry]:\n",
    "        \"\"\"\n",
    "        Hauptmethode: Crawlt alle Artikel und gibt strukturierte Daten zurück\n",
    "        \"\"\"\n",
    "        logger.info(\"Starte DSGVO Crawler...\")\n",
    "\n",
    "        # 1. Parse Übersichtsseite für Artikel-Links\n",
    "        artikel_links = self.parse_overview_page()\n",
    "        if not artikel_links:\n",
    "            logger.error(\"Keine Artikel gefunden\")\n",
    "            return []\n",
    "\n",
    "        all_entries = []\n",
    "\n",
    "        # 2. Parse jeden Artikel\n",
    "        for i, (artikel_url, context) in enumerate(artikel_links, 1):\n",
    "            logger.info(f\"Verarbeite Artikel {i}/{len(artikel_links)}: {artikel_url}\")\n",
    "\n",
    "            try:\n",
    "                entries = self.parse_article_page(artikel_url, context)\n",
    "                all_entries.extend(entries)\n",
    "                logger.info(f\"Artikel {i}: {len(entries)} Einträge extrahiert\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Fehler beim Verarbeiten von {artikel_url}: {e}\")\n",
    "                continue\n",
    "\n",
    "        logger.info(f\"Crawling abgeschlossen. Insgesamt {len(all_entries)} Einträge extrahiert.\")\n",
    "        return all_entries\n",
    "\n",
    "    def save_to_jsonl(self, entries: List[DSGVOEntry], output_path: str):\n",
    "        \"\"\"\n",
    "        Speichert die Einträge als JSONL-Datei\n",
    "        \"\"\"\n",
    "        # Erstelle Output-Verzeichnis falls nicht vorhanden\n",
    "        output_file = Path(output_path)\n",
    "        output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        logger.info(f\"Speichere {len(entries)} Einträge in: {output_path}\")\n",
    "\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            for entry in entries:\n",
    "                json_line = json.dumps(entry.to_dict(), ensure_ascii=False)\n",
    "                f.write(json_line + '\\n')\n",
    "\n",
    "        logger.info(f\"Erfolgreich gespeichert: {output_path}\")\n",
    "\n",
    "    def run_full_crawl(self, output_path: str = None):\n",
    "        \"\"\"\n",
    "        Führt einen vollständigen Crawl-Durchlauf aus\n",
    "        \"\"\"\n",
    "        if output_path is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y-%m-%d_%H%M\")\n",
    "            output_path = f\"../data/output/dsgvo_crawled_{timestamp}.jsonl\"\n",
    "\n",
    "        try:\n",
    "            # Crawle alle Artikel\n",
    "            entries = self.crawl_all_articles()\n",
    "\n",
    "            if entries:\n",
    "                # Speichere Ergebnisse\n",
    "                self.save_to_jsonl(entries, output_path)\n",
    "\n",
    "                # Statistiken\n",
    "                logger.info(\"=== CRAWLING STATISTIKEN ===\")\n",
    "                logger.info(f\"Gesamt Einträge: {len(entries)}\")\n",
    "\n",
    "                # Gruppiere nach Artikeln\n",
    "                artikel_count = len(set((e.artikel_nr for e in entries)))\n",
    "                logger.info(f\"Artikel verarbeitet: {artikel_count}\")\n",
    "\n",
    "                # Gruppiere nach Kapiteln\n",
    "                kapitel_count = len(set((e.kapitel_nr for e in entries)))\n",
    "                logger.info(f\"Kapitel verarbeitet: {kapitel_count}\")\n",
    "\n",
    "                return output_path\n",
    "            else:\n",
    "                logger.error(\"Keine Einträge zum Speichern gefunden\")\n",
    "                return None\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Fehler beim Crawling: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12627fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-11 18:04:38,986 - INFO - Starte DSGVO Crawler...\n",
      "2025-08-11 18:04:38,987 - INFO - Lade Seite: https://dsgvo-gesetz.de (Versuch 1)\n",
      "2025-08-11 18:04:40,189 - INFO - Gefunden: Kapitel 1 - Allgemeine Bestimmungen\n",
      "2025-08-11 18:04:40,198 - INFO - Gefunden: Kapitel 2 - Grundsätze\n",
      "2025-08-11 18:04:40,203 - INFO - Gefunden: Kapitel 3 - Rechte der betroffenen Person\n",
      "2025-08-11 18:04:40,208 - INFO - Gefunden: Abschnitt 1 - Transparenz und Modalitäten\n",
      "2025-08-11 18:04:40,212 - INFO - Gefunden: Abschnitt 2 - Informationspflicht und Recht auf Auskunft zu personenbezogenen Daten\n",
      "2025-08-11 18:04:40,216 - INFO - Gefunden: Abschnitt 3 - Berichtigung und Löschung\n",
      "2025-08-11 18:04:40,219 - INFO - Gefunden: Abschnitt 4 - Widerspruchsrecht und automatisierte Entscheidungsfindung im Einzelfall\n",
      "2025-08-11 18:04:40,221 - INFO - Gefunden: Abschnitt 5 - Beschränkungen\n",
      "2025-08-11 18:04:40,222 - INFO - Gefunden: Kapitel 4 - Verantwortlicher und Auftragsverarbeiter\n",
      "2025-08-11 18:04:40,224 - INFO - Gefunden: Abschnitt 1 - Allgemeine Pflichten\n",
      "2025-08-11 18:04:40,226 - INFO - Gefunden: Abschnitt 2 - Sicherheit personenbezogener Daten\n",
      "2025-08-11 18:04:40,228 - INFO - Gefunden: Abschnitt 3 - Datenschutz-Folgenabschätzung und vorherige Konsultation\n",
      "2025-08-11 18:04:40,229 - INFO - Gefunden: Abschnitt 4 - Datenschutzbeauftragter\n",
      "2025-08-11 18:04:40,231 - INFO - Gefunden: Abschnitt 5 - Verhaltensregeln und Zertifizierung\n",
      "2025-08-11 18:04:40,232 - INFO - Gefunden: Kapitel 5 - Übermittlungen personenbezogener Daten an Drittländer oder an internationale Organisationen\n",
      "2025-08-11 18:04:40,234 - INFO - Gefunden: Kapitel 6 - Unabhängige Aufsichtsbehörden\n",
      "2025-08-11 18:04:40,235 - INFO - Gefunden: Abschnitt 1 - Unabhängigkeit\n",
      "2025-08-11 18:04:40,236 - INFO - Gefunden: Abschnitt 2 - Zuständigkeit, Aufgaben und Befugnisse\n",
      "2025-08-11 18:04:40,238 - INFO - Gefunden: Kapitel 7 - Zusammenarbeit und Kohärenz\n",
      "2025-08-11 18:04:40,239 - INFO - Gefunden: Abschnitt 1 - Zusammenarbeit\n",
      "2025-08-11 18:04:40,240 - INFO - Gefunden: Abschnitt 2 - Kohärenz\n",
      "2025-08-11 18:04:40,241 - INFO - Gefunden: Abschnitt 3 - Europäischer Datenschutzausschuss\n",
      "2025-08-11 18:04:40,243 - INFO - Gefunden: Kapitel 8 - Rechtsbehelfe, Haftung und Sanktionen\n",
      "2025-08-11 18:04:40,244 - INFO - Gefunden: Kapitel 9 - Vorschriften für besondere Verarbeitungssituationen\n",
      "2025-08-11 18:04:40,246 - INFO - Gefunden: Kapitel 10 - Delegierte Rechtsakte und Durchführungsrechtsakte\n",
      "2025-08-11 18:04:40,247 - INFO - Gefunden: Kapitel 11 - Schlussbestimmungen\n",
      "2025-08-11 18:04:40,248 - INFO - Insgesamt 99 Artikel gefunden\n",
      "2025-08-11 18:04:40,249 - INFO - Verarbeite Artikel 1/99: https://dsgvo-gesetz.de/art-1-dsgvo/\n",
      "2025-08-11 18:04:40,249 - INFO - Lade Seite: https://dsgvo-gesetz.de/art-1-dsgvo/ (Versuch 1)\n",
      "2025-08-11 18:04:41,350 - INFO - Parse Artikel 1: Gegenstand und Ziele\n",
      "2025-08-11 18:04:41,351 - INFO - Artikel 1: 3 Einträge extrahiert\n",
      "2025-08-11 18:04:41,353 - INFO - Verarbeite Artikel 2/99: https://dsgvo-gesetz.de/art-2-dsgvo/\n",
      "2025-08-11 18:04:41,353 - INFO - Lade Seite: https://dsgvo-gesetz.de/art-2-dsgvo/ (Versuch 1)\n",
      "2025-08-11 18:04:42,412 - INFO - Parse Artikel 2: Sachlicher Anwendungsbereich\n",
      "2025-08-11 18:04:42,415 - INFO - Artikel 2: 8 Einträge extrahiert\n",
      "2025-08-11 18:04:42,417 - INFO - Verarbeite Artikel 3/99: https://dsgvo-gesetz.de/art-3-dsgvo/\n",
      "2025-08-11 18:04:42,419 - INFO - Lade Seite: https://dsgvo-gesetz.de/art-3-dsgvo/ (Versuch 1)\n",
      "2025-08-11 18:04:43,567 - INFO - Parse Artikel 3: Räumlicher Anwendungsbereich\n",
      "2025-08-11 18:04:43,569 - INFO - Artikel 3: 5 Einträge extrahiert\n",
      "2025-08-11 18:04:43,570 - INFO - Verarbeite Artikel 4/99: https://dsgvo-gesetz.de/art-4-dsgvo/\n",
      "2025-08-11 18:04:43,570 - INFO - Lade Seite: https://dsgvo-gesetz.de/art-4-dsgvo/ (Versuch 1)\n",
      "2025-08-11 18:04:44,628 - INFO - Parse Artikel 4: Begriffsbestimmungen\n",
      "2025-08-11 18:04:44,630 - INFO - Artikel 4: 33 Einträge extrahiert\n",
      "2025-08-11 18:04:44,630 - INFO - Verarbeite Artikel 5/99: https://dsgvo-gesetz.de/art-5-dsgvo/\n",
      "2025-08-11 18:04:44,631 - INFO - Lade Seite: https://dsgvo-gesetz.de/art-5-dsgvo/ (Versuch 1)\n",
      "2025-08-11 18:04:45,681 - INFO - Parse Artikel 5: Grundsätze für die Verarbeitung personenbezogener Daten\n",
      "2025-08-11 18:04:45,682 - INFO - Artikel 5: 8 Einträge extrahiert\n",
      "2025-08-11 18:04:45,684 - INFO - Verarbeite Artikel 6/99: https://dsgvo-gesetz.de/art-6-dsgvo/\n",
      "2025-08-11 18:04:45,685 - INFO - Lade Seite: https://dsgvo-gesetz.de/art-6-dsgvo/ (Versuch 1)\n",
      "2025-08-11 18:04:46,746 - INFO - Parse Artikel 6: Rechtmäßigkeit der Verarbeitung\n",
      "2025-08-11 18:04:46,750 - INFO - Artikel 6: 17 Einträge extrahiert\n",
      "2025-08-11 18:04:46,751 - INFO - Verarbeite Artikel 7/99: https://dsgvo-gesetz.de/art-7-dsgvo/\n",
      "2025-08-11 18:04:46,751 - INFO - Lade Seite: https://dsgvo-gesetz.de/art-7-dsgvo/ (Versuch 1)\n",
      "2025-08-11 18:04:47,811 - INFO - Parse Artikel 7: Bedingungen für die Einwilligung\n",
      "2025-08-11 18:04:47,814 - INFO - Artikel 7: 4 Einträge extrahiert\n",
      "2025-08-11 18:04:47,815 - INFO - Verarbeite Artikel 8/99: https://dsgvo-gesetz.de/art-8-dsgvo/\n",
      "2025-08-11 18:04:47,816 - INFO - Lade Seite: https://dsgvo-gesetz.de/art-8-dsgvo/ (Versuch 1)\n",
      "2025-08-11 18:04:48,879 - INFO - Parse Artikel 8: Bedingungen für die Einwilligung eines Kindes in Bezug auf Dienste der Informationsgesellschaft\n",
      "2025-08-11 18:04:48,881 - INFO - Artikel 8: 3 Einträge extrahiert\n",
      "2025-08-11 18:04:48,881 - INFO - Verarbeite Artikel 9/99: https://dsgvo-gesetz.de/art-9-dsgvo/\n",
      "2025-08-11 18:04:48,883 - INFO - Lade Seite: https://dsgvo-gesetz.de/art-9-dsgvo/ (Versuch 1)\n",
      "2025-08-11 18:04:49,965 - INFO - Parse Artikel 9: Verarbeitung besonderer Kategorien personenbezogener Daten\n",
      "2025-08-11 18:04:49,967 - INFO - Artikel 9: 14 Einträge extrahiert\n",
      "2025-08-11 18:04:49,968 - INFO - Verarbeite Artikel 10/99: https://dsgvo-gesetz.de/art-10-dsgvo/\n",
      "2025-08-11 18:04:49,968 - INFO - Lade Seite: https://dsgvo-gesetz.de/art-10-dsgvo/ (Versuch 1)\n",
      "2025-08-11 18:04:51,037 - INFO - Parse Artikel 10: Verarbeitung von personenbezogenen Daten über strafrechtliche Verurteilungen und Straftaten\n",
      "2025-08-11 18:04:51,047 - INFO - Artikel 10: 2 Einträge extrahiert\n",
      "2025-08-11 18:04:51,051 - INFO - Verarbeite Artikel 11/99: https://dsgvo-gesetz.de/art-11-dsgvo/\n",
      "2025-08-11 18:04:51,055 - INFO - Lade Seite: https://dsgvo-gesetz.de/art-11-dsgvo/ (Versuch 1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Erstelle Crawler-Instanz\n",
    "    crawler = DSGVOCrawler()\n",
    "\n",
    "    # Führe vollständigen Crawl aus\n",
    "    output_file = crawler.run_full_crawl()\n",
    "\n",
    "    if output_file:\n",
    "        print(f\"\\nCrawling erfolgreich abgeschlossen!\")\n",
    "        print(f\"Ausgabedatei: {output_file}\")\n",
    "        print(f\"Log-Datei: ../data/logs/crawler.log\")\n",
    "\n",
    "        # Kurze Vorschau der ersten Einträge\n",
    "        with open(output_file, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()[:3]\n",
    "            print(f\"\\nErste {len(lines)} Einträge:\")\n",
    "            for i, line in enumerate(lines, 1):\n",
    "                entry = json.loads(line)\n",
    "                print(f\"{i}. Artikel {entry['Artikel_nr']}, Absatz {entry['Absatz_nr']}, Satz {entry['Satz_nr']}\")\n",
    "                print(f\"   Text: {entry['Text'][:100]}...\")\n",
    "    else:\n",
    "        print(\"Crawling fehlgeschlagen!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
