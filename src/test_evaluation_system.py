#!/usr/bin/env python3
"""
Testskript fÃ¼r das Evaluierungssystem.

Testet alle Evaluierungskomponenten einzeln ohne komplexe RAG-Tests.
"""

import os
import sys
import json
from pathlib import Path

# Projekt-Root zum Python-Pfad hinzufÃ¼gen
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.evaluations import (
    RetrievalEvaluator, GenerationEvaluator,
    PerformanceEvaluator
)


def test_retrieval_evaluator():
    """Testet den RetrievalEvaluator."""
    print("ğŸ” Teste RetrievalEvaluator...")

    evaluator = RetrievalEvaluator()

    # Test-Daten
    predictions = [
        ["doc1", "doc2", "doc3"],
        ["doc2", "doc4", "doc5"],
        ["doc1", "doc3", "doc6"]
    ]

    ground_truth = [
        ["doc1", "doc2"],
        ["doc2", "doc3"],
        ["doc1", "doc4"]
    ]

    results = evaluator.evaluate(predictions, ground_truth)

    print(f"  âœ… Precision@5: {results['precision@5']:.3f}")
    print(f"  âœ… Recall@5: {results['recall@5']:.3f}")
    print(f"  âœ… F1@5: {results['f1@5']:.3f}")
    print(f"  âœ… MRR: {results['mrr']:.3f}")
    print(f"  âœ… NDCG@5: {results['ndcg@5']:.3f}")

    return results


def test_generation_evaluator():
    """Testet den GenerationEvaluator."""
    print("ğŸ“ Teste GenerationEvaluator...")

    evaluator = GenerationEvaluator()

    # Test-Daten
    predictions = [
        "Die maximale GeldbuÃŸe betrÃ¤gt 20 Millionen Euro.",
        "Betroffene Personen haben verschiedene Rechte.",
        "Eine Einwilligung muss freiwillig sein."
    ]

    ground_truth = [
        "Die maximale GeldbuÃŸe nach DSGVO betrÃ¤gt 20 Millionen Euro oder 4% des Jahresumsatzes.",
        "Betroffene haben Rechte auf Auskunft, Berichtigung und LÃ¶schung.",
        "Eine Einwilligung muss freiwillig, informiert und eindeutig sein."
    ]

    results = evaluator.evaluate(predictions, ground_truth)

    print(f"  âœ… ROUGE-L: {results['rouge_l']:.3f}")
    print(f"  âœ… BLEU: {results['bleu']:.3f}")
    print(f"  âœ… Exact Match: {results['exact_match']:.3f}")
    print(f"  âœ… Semantic Similarity: {results['semantic_similarity']:.3f}")

    return results


def test_performance_evaluator():
    """Testet den PerformanceEvaluator."""
    print("âš¡ Teste PerformanceEvaluator...")

    evaluator = PerformanceEvaluator()

    # Test-Daten
    latencies = [0.5, 0.8, 0.6, 1.2, 0.9]
    timestamps = [1.0, 2.0, 3.0, 4.0, 5.0]

    results = evaluator.evaluate(
        [], [],  # Dummy-Daten
        latencies=latencies,
        timestamps=timestamps
    )

    print(f"  âœ… Durchschnittliche Latenz: {results['avg_latency']:.3f}s")
    print(f"  âœ… Median Latenz: {results['median_latency']:.3f}s")
    print(f"  âœ… P95 Latenz: {results['p95_latency']:.3f}s")
    print(f"  âœ… Throughput: {results['throughput_qps']:.1f} QPS")

    return results


def test_simple_rag_evaluator():
    """Testet den RAGEvaluator mit einfachen Daten."""
    print("ğŸ¯ Teste RAGEvaluator (vereinfacht)...")

    from src.evaluations import RAGEvaluator
    evaluator = RAGEvaluator()

    # Einfache Test-Daten ohne Kategorien
    predictions = [
        {
            "question": "Was ist die maximale GeldbuÃŸe?",
            "answer": "Die maximale GeldbuÃŸe betrÃ¤gt 20 Millionen Euro.",
            "retrieved_contexts": [
                {"chunk_id": "1", "text": "Art. 83 DSGVO regelt GeldbuÃŸen von bis zu 20 Millionen Euro."},
            ],
            "query_time": 0.8,
            "timestamp": 1234567890
        }
    ]

    ground_truth = [
        {
            "question": "Was ist die maximale GeldbuÃŸe?",
            "gold_answer": "Die maximale GeldbuÃŸe nach DSGVO betrÃ¤gt 20 Millionen Euro oder 4% des Jahresumsatzes.",
            "relevant_chunks": ["1"]
        }
    ]

    try:
        results = evaluator.evaluate(predictions, ground_truth)

        print(f"  âœ… RAG Score: {results.get('rag_score', 0.0):.3f}")
        print(f"  âœ… Quality Score: {results.get('quality_score', 0.0):.3f}")
        print(f"  âœ… Efficiency Score: {results.get('efficiency_score', 0.0):.3f}")
        print(f"  âœ… Overall Score: {results.get('overall_score', 0.0):.3f}")
        print(f"  âœ… Faithfulness: {results.get('faithfulness', 0.0):.3f}")
        print(f"  âœ… Groundedness: {results.get('groundedness', 0.0):.3f}")

        return results
    except Exception as e:
        print(f"  âŒ RAGEvaluator Test fehlgeschlagen: {e}")
        return None


def test_component_loader():
    """Testet den erweiterten ComponentLoader."""
    print("ğŸ”§ Teste ComponentLoader...")

    from src.core.component_loader import ComponentLoader

    loader = ComponentLoader()

    # VerfÃ¼gbare Komponenten anzeigen
    available = loader.get_available_components()
    print(f"  âœ… VerfÃ¼gbare Evaluatoren: {available['evaluators']}")

    # Evaluator laden
    evaluator_config = {"type": "retrieval", "k_values": [1, 3, 5]}
    evaluator = loader.load_evaluator(evaluator_config)

    print(f"  âœ… Evaluator geladen: {evaluator}")

    return evaluator


def main():
    """Hauptfunktion fÃ¼r alle Tests."""
    print("ğŸš€ Starte Evaluierungssystem-Tests...\n")

    test_results = {}

    # Einzelne Evaluatoren testen
    test_results["retrieval"] = test_retrieval_evaluator()
    print()

    test_results["generation"] = test_generation_evaluator()
    print()

    test_results["performance"] = test_performance_evaluator()
    print()

    test_results["rag_simple"] = test_simple_rag_evaluator()
    print()

    # ComponentLoader testen
    test_results["component_loader"] = test_component_loader()
    print()

    # Zusammenfassung
    print("ğŸ“Š Test-Zusammenfassung:")
    print("=" * 50)

    for test_name, result in test_results.items():
        status = "âœ… BESTANDEN" if result is not None else "âŒ FEHLGESCHLAGEN"
        print(f"{test_name:20} {status}")

    print("\nğŸ‰ Evaluierungssystem-Tests abgeschlossen!")

    return test_results


if __name__ == "__main__":
    main()