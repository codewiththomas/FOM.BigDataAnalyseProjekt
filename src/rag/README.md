# ResearchRAG - Modular RAG Evaluation System

A modular, configurable RAG (Retrieval-Augmented Generation) system designed for scientific evaluation and experimentation with different components.

## ğŸ¯ Features

- **Modular Architecture**: Easily swap LLMs, embeddings, chunking strategies, and retrieval methods
- **YAML Configuration**: Simple configuration files for different experiments
- **Automatic Evaluation**: Built-in evaluation metrics (Precision, Recall, F1, RAGAS-style, timing)
- **DSGVO Dataset**: Pre-configured for DSGVO legal text evaluation
- **Multiple Models**: Support for OpenAI API and local models (GPT-OSS-20B, Mixtral-7B, Qwen3, Llama3-Sauerkraut)

## ğŸ—ï¸ Architecture

```
ResearchRAG/
â”œâ”€â”€ interfaces.py      # Abstract interfaces for all components
â”œâ”€â”€ pipeline.py        # Main RAG pipeline orchestrator
â”œâ”€â”€ config.py          # YAML configuration manager
â”œâ”€â”€ factory.py         # Component factory
â”œâ”€â”€ llms.py           # LLM implementations (OpenAI, Local)
â”œâ”€â”€ embeddings.py     # Embedding implementations (OpenAI, Sentence-Transformers)
â”œâ”€â”€ chunking.py       # Chunking strategies (Fixed-size, Semantic)
â”œâ”€â”€ retrieval.py      # Retrieval methods (Vector similarity, Hybrid)
â”œâ”€â”€ evaluation.py     # Evaluation metrics
â”œâ”€â”€ dataset.py        # DSGVO dataset management
â”œâ”€â”€ evaluator.py      # Main evaluation orchestrator
â””â”€â”€ main.py           # CLI interface
```

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
pip install -r requirements_rag.txt
```

### 2. Set Environment Variables

```bash
export OPENAI_API_KEY="your-openai-api-key"
```

### 3. Run Baseline Evaluation

```bash
python -m rag.main --config configs/baseline.yaml --dataset data/output/dsgvo_crawled_2025-08-14_1535.jsonl
```

## ğŸ“‹ Configuration

### Baseline Configuration (GPT-4o-mini)

```yaml
llm:
  type: openai
  model: gpt-4o-mini
  temperature: 0.1

embedding:
  type: openai
  model: text-embedding-ada-002

chunking:
  type: fixed-size
  chunk_size: 1000
  chunk_overlap: 200

retrieval:
  type: vector-similarity
  top_k: 5
```

### Local Models Configuration

```yaml
llm:
  type: local
  model_name: gpt-oss-20b
  endpoint: http://localhost:8000
  api_type: ollama

embedding:
  type: sentence-transformers
  model_name: all-MiniLM-L6-v2
  device: cpu
```

## ğŸ”¬ Experiments

### 1. Different Language Models

- **GPT-4o-mini**: Baseline reference model
- **GPT-OSS-20B**: Open-source 20B parameter model
- **Mixtral-7B**: Mixture of experts model
- **Qwen3**: Alibaba's Qwen model
- **Llama3-Sauerkraut**: German-optimized Llama model

### 2. Embedding & Chunking Experiments

- **Fixed-size chunking**: Different chunk sizes (500, 1000, 2000)
- **Semantic chunking**: Natural boundary-based chunking
- **Sentence-based**: Sentence-level chunking
- **Embedding models**: OpenAI vs. Sentence-Transformers

### 3. Retrieval Techniques

- **Vector similarity**: Different top-k values (3, 5, 10)
- **Similarity thresholds**: Filtering by relevance
- **Hybrid retrieval**: Vector + keyword combinations

## ğŸ“Š Evaluation Metrics

### Traditional Metrics
- **Precision**: How many retrieved chunks are relevant
- **Recall**: How many relevant chunks were retrieved
- **F1**: Harmonic mean of precision and recall

### RAG-Specific Metrics
- **Faithfulness**: How much response is based on retrieved chunks
- **Answer Relevance**: How relevant response is to query
- **Context Relevance**: How relevant retrieved chunks are to query

### Performance Metrics
- **Query Time**: Total time per query
- **Tokens per Second**: Generation speed
- **Response Length**: Output size

## ğŸ® Usage Examples

### Run Different Configurations

```bash
# Baseline evaluation
python -m rag.main --config configs/baseline.yaml --dataset data/output/dsgvo_crawled_2025-08-14_1535.jsonl

# Local models evaluation
python -m rag.main --config configs/local_models.yaml --dataset data/output/dsgvo_crawled_2025-08-14_1535.jsonl

# Chunking experiments
python -m rag.main --config configs/chunking_experiments.yaml --dataset data/output/dsgvo_crawled_2025-08-14_1535.jsonl

# Retrieval experiments
python -m rag.main --config configs/retrieval_experiments.yaml --dataset data/output/dsgvo_crawled_2025-08-14_1535.jsonl
```

### Custom Evaluation

```bash
# Evaluate with 100 QA pairs
python -m rag.main --config configs/baseline.yaml --dataset data/output/dsgvo_crawled_2025-08-14_1535.jsonl --num-qa 100

# Run without saving results
python -m rag.main --config configs/baseline.yaml --dataset data/output/dsgvo_crawled_2025-08-14_1535.jsonl --no-save

# Verbose logging
python -m rag.main --config configs/baseline.yaml --dataset data/output/dsgvo_crawled_2025-08-14_1535.jsonl --verbose
```

## ğŸ”§ Customization

### Adding New LLM Types

1. Implement the `LLMInterface` in `llms.py`
2. Add to `LLMFactory.create_llm()`
3. Create configuration file

### Adding New Embedding Models

1. Implement the `EmbeddingInterface` in `embeddings.py`
2. Add to `EmbeddingFactory.create_embedding()`
3. Update configuration

### Adding New Evaluation Metrics

1. Implement the `EvaluationInterface` in `evaluation.py`
2. Add to `EvaluationManager._setup_evaluators()`
3. Enable in configuration

## ğŸ“ Output Files

The system generates:
- `evaluation_results_YYYYMMDD_HHMMSS.json`: Detailed results for each QA pair
- `evaluation_summary_YYYYMMDD_HHMMSS.json`: Summary statistics and metrics

## ğŸš§ Current Limitations

- **Local LLMs**: Placeholder implementations - need integration with Ollama, vLLM, etc.
- **Vector Search**: Simplified similarity calculation - could use FAISS, Chroma, etc.
- **Advanced Metrics**: RAGAS implementation is simplified - could use full RAGAS library

## ğŸ”® Future Enhancements

- **Real Vector Database**: Integration with FAISS, Chroma, Pinecone
- **Advanced Chunking**: NLP-based semantic chunking
- **Full RAGAS**: Complete RAGAS metric implementation
- **Batch Processing**: Parallel evaluation for faster results
- **Web Interface**: Web-based configuration and monitoring

## ğŸ¤ Contributing

1. Follow the modular interface design
2. Add comprehensive logging
3. Include error handling
4. Update configuration examples
5. Add tests for new components

## ğŸ“„ License

This project is part of the FOM Big Data Analysis Project.
